# ğŸ§  Fully Local RAG System (Phi3 + FAISS)

A production-style Retrieval-Augmented Generation (RAG) system built using:

- ğŸŸ¢ Phi-3 (local LLM via Ollama)
- ğŸŸ¢ HuggingFace Sentence Transformers (local embeddings)
- ğŸŸ¢ FAISS (vector similarity search)
- ğŸŸ¢ LangChain (orchestration layer)

Optimized for low-resource systems (8GB RAM).

---

## ğŸš€ Problem Statement

Large Language Models hallucinate when asked questions outside their training data.

This project solves that problem using Retrieval-Augmented Generation (RAG):

Instead of relying only on model memory,
we retrieve relevant documents and ground the response.

---

## ğŸ—ï¸ Architecture

User Query  
â†“  
Embedding (all-MiniLM-L6-v2)  
â†“  
FAISS Vector Search (Top-K Retrieval)  
â†“  
Phi-3 Local LLM (Ollama)  
â†“  
Grounded Answer  

---

## ğŸ”¬ Key Engineering Decisions

### âœ… Why Local Embeddings?
- Avoid API cost
- Faster inference
- No dependency on external services

### âœ… Why Phi-3?
- Lightweight (works on 8GB RAM)
- Good reasoning performance
- Efficient inference

### âœ… Why FAISS?
- Efficient Approximate Nearest Neighbor (ANN) search
- Scales to large vector datasets

---

## ğŸ“¦ Features

- Document chunking with overlap
- Semantic vector search
- Top-K retrieval
- Hallucination reduction via grounded prompts
- Fully offline execution
- Optimized for resource-constrained hardware

---

## ğŸ› ï¸ Installation

```bash
pip install -r requirements.txt
ollama pull phi3
python app.py
